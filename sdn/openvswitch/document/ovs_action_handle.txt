1. actions的分类
a. 改变某些值
b. 寄存器操作
c. stack操作
d. multipatch,group,go_table,resubmit
e. controller
f. output

a和b通常长都转换为内核态的OVS_ACTION_ATTR_SET和OVS_ACTION_ATTR_PUSH/POP_VLAN
d主要是是多级操作, 包含多个或者一个actions集操作
f最后都是output到某个port, output会根据前面改变的值生成内核的actions以及设置wildcard值

2. 内核态flow
内核态的flow也分为key项和actions
key就是miss flow的key(L2,3,4的各个数据), actions就是查找到的用户态flow的actions。
那么不同的报文流可能对应相同的用户态flow, 从而产生多个内核态flow. 性能下降
megaflow给内核态的流表增加了masks选项. 
in_port, dl_type, priority始终是为ff, 必须精确匹配
其他的值
1). 如果在匹配的用户态flow的match项存在的时候, 也设为全ff或者用户态该match项的mask.  
2). 如果在匹配的用户态flow的actions中有修改该值的时候设置为全ff 


3. actions
用户态的actions翻译成内核态的actions
do_xlate_actions(const struct ofpact *ofpacts, size_t ofpacts_len, struct xlate_ctx *ctx)
{
	struct flow_wildcards *wc = &ctx->xout->wc;
    struct flow *flow = &ctx->xin->flow;

	OFPACT_FOR_EACH (a, ofpacts, ofpacts_len) {
		switch (a->type) {
			......
			......
		}
	}
}

a. 改变值
1). 直接设置  mod_dl_src:mac
	case OFPACT_SET_ETH_SRC:
			/*mask wc*/
            memset(&wc->masks.dl_src, 0xff, sizeof wc->masks.dl_src);
			/*设置flow值*/
            memcpy(flow->dl_src, ofpact_get_SET_ETH_SRC(a)->mac, ETH_ADDR_LEN);
            break;
2). setfield  set_field:value->dst
	case OFPACT_SET_FIELD:
            set_field = ofpact_get_SET_FIELD(a);
            mf = set_field->field;
			/*mask wc*/
			mf_mask_field_and_prereqs(mf, &wc->masks);
			/*设置flow值*/
            mf_set_flow_value(mf, &set_field->value, flow)

b. 寄存器操作, L2,3,4的每个值都可以表示为一个寄存器, 之间做mov, 或者单独load
1). mov  move:src[start..end]->dst[start..end]
	case OFPACT_REG_MOVE:
            nxm_execute_reg_move(ofpact_get_REG_MOVE(a), flow, wc);
			{
				/*mask src dst*/
				mf_mask_field_and_prereqs(move->dst.field, &wc->masks);
    			mf_mask_field_and_prereqs(move->src.field, &wc->masks);

				/*获取src 设置dst*/
    			mf_get_value(move->dst.field, flow, &dst_value);
    			mf_get_value(move->src.field, flow, &src_value);
    			bitwise_copy(&src_value, move->src.field->n_bytes, move->src.ofs,
                 &dst_value, move->dst.field->n_bytes, move->dst.ofs,
                 move->src.n_bits);
    			mf_set_flow_value(move->dst.field, &dst_value, flow);
			}

2).load load:value->dst[start..end]
	case OFPACT_REG_LOAD:
            nxm_execute_reg_load(ofpact_get_REG_LOAD(a), flow, wc);
			{
				/*mask dst */
				mf_mask_field_and_prereqs(load->dst.field, &wc->masks);
				/*set dst*/
    			mf_write_subfield_flow(&load->dst, &load->subvalue, flow);
			}

c. stack操作 将值域先存于一个栈中, 后续pop到一个值域
1). push push:src[start..end]
	case OFPACT_STACK_PUSH:
            nxm_execute_stack_push(ofpact_get_STACK_PUSH(a), flow, wc, &ctx->stack);
			{
				/*设置src mask*/
				memset(&mask_value, 0xff, sizeof mask_value);
    			mf_write_subfield_flow(&push->subfield, &mask_value, &wc->masks);

				/*读取存于stack*/
    			mf_read_subfield(&push->subfield, flow, &dst_value);
    			nx_stack_push(stack, &dst_value);
			}

2). pop pop:dst[start..end]
	case OFPACT_STACK_PUSH:
		nxm_execute_stack_push(ofpact_get_STACK_PUSH(a), flow, wc, &ctx->stack);
		{
			/*stack中pop*/
			src_value = nx_stack_pop(stack);
			
			/*设置dst mask*/
			memset(&mask_value, 0xff, sizeof mask_value);
        	mf_write_subfield_flow(&pop->subfield, &mask_value, &wc->masks);
        	mf_write_subfield_flow(&pop->subfield, src_value, flow)
		}

d. multipatch,group,go_table,resubmit: 再查表或者多个actions集合选择
1). multipath: 多路径hash选择, 把路径link值填写到reg里面去  multipath(fields, basis, algorithm, n_links, arg, dst[start..end])
	case OFPACT_MULTIPATH:
            multipath_execute(ofpact_get_MULTIPATH(a), flow, wc);
			{
				/*计算hash 获取link*/
				uint32_t hash = flow_hash_fields(flow, mp->fields, mp->basis);
    			uint16_t link = multipath_algorithm(hash, mp->algorithm,
                                        mp->max_link + 1, mp->arg);

				/*mask dst reg*/
    			flow_mask_hash_fields(flow, wc, mp->fields);
				/*link值写进dst*/
    			nxm_reg_load(&mp->dst, link, flow, wc);
			}
通常multpatch 跟go_table或者resubmit一起用. 再去根据里面的流表(含有reg的match项)进行发送

2). group: 多个actions集合进行策略选择
	case OFPACT_GROUP:
            xlate_group_action(ctx, ofpact_get_GROUP(a)->group_id)) 
			{
				/*根据group id选择group*/
            	got_group = group_dpif_lookup(ctx->xbridge->ofproto, group_id, &group);
        			if (got_group) {
            			xlate_group_action__(ctx, group);
        	} 
3). goto_table 去另外一个table查询 goto_table:table
	 case OFPACT_GOTO_TABLE: {
            struct ofpact_goto_table *ogt = ofpact_get_GOTO_TABLE(a);

            xlate_table_action(ctx, ctx->xin->flow.in_port.ofp_port,
                               ogt->table_id, true, true);
	}

4). resubmit重新提交给某个table查询,可以修改in_port  resubmit([port],[table])
	case OFPACT_RESUBMIT:
            xlate_ofpact_resubmit(ctx, ofpact_get_RESUBMIT(a));
			{
				/*获取port*/
				in_port = resubmit->in_port;
    			if (in_port == OFPP_IN_PORT) {
        			in_port = ctx->xin->flow.in_port.ofp_port;
    			}

				/*获取table*/
    			table_id = resubmit->table_id;
				if (table_id == 255) {
        			table_id = ctx->table_id;
    			}

    			xlate_table_action(ctx, in_port, table_id, may_packet_in,
                       honor_table_miss);
			}

e. controller: 如果在table中没查找到, 默认行为是controller或者查找到指定controller的行为
controller(key=value...):发送packet in给controller
case OFPACT_CONTROLLER:
            controller = ofpact_get_CONTROLLER(a);
            execute_controller_action(ctx, controller->max_len,
                                      controller->reason,
                                      controller->controller_id);
			{
				/*标记为slow 获取packt*/
				ctx->xout->slow |= SLOW_CONTROLLER;
				packet = ofpbuf_clone(ctx->xin->packet);
				/*创建pin消息 发送packet in*/
				pin = xmalloc(sizeof *pin);
    			pin->up.packet_len = ofpbuf_size(packet);
    			pin->up.packet = ofpbuf_steal_data(packet);
    			pin->up.reason = reason;
    			pin->up.table_id = ctx->table_id;
				ofproto_dpif_send_packet_in(ctx->xbridge->ofproto, pin);
			} 

f. output: 指定发送的ofport, in_port, local等 output:port
	case OFPACT_OUTPUT:
            xlate_output_action(ctx, ofpact_get_OUTPUT(a)->port, ofpact_get_OUTPUT(a)->max_len, true);
			===>compose_output_action(ctx, port)===>commit_odp_actions
			{
				/*设置L2,3,4的改变以及port等的内核态flow action*/
				commit_set_ether_addr_action(flow, base, odp_actions, wc);
    			slow = commit_set_nw_action(flow, base, odp_actions, wc);
    			commit_set_port_action(flow, base, odp_actions, wc);
    			commit_mpls_action(flow, base, odp_actions, wc);
    			commit_vlan_action(flow->vlan_tci, base, odp_actions, wc);
    			commit_set_priority_action(flow, base, odp_actions, wc);
    			commit_set_pkt_mark_action(flow, base, odp_actions, wc);
			}
				
