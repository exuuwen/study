1. API
rcu_read_lock():Begin an RCU critical section.
rcu_read_unlock():Complete an RCU critical section.
synchronize_rcu():Wait for existing RCU critical sections to complete.

call_rcu(callback, arguments...):Call the callback when existing RCU critical sections complete.
rcu_dereference(pointer):Signal the intent to deference a pointer in an RCU critical section.
rcu_assign_pointer(pointer_addr, pointer):Assign a value to a pointer that is read in RCU critical sections.

2. Why rcu
RCU provides higher performance than traditional read-write locking and can make it easier to reason about dead-lock.
Another reason to choose RCU instead of a read-write lock is deadlock immunity. The only way for RCU to deadlock is if a thread blocks waiting for synchronize_rcu in a RCU critical section.

3. example
1) pointer protect
syscall_t *table;
spinlock_t table_lock;

int invoke_syscall(int number, void *args...)
{
	syscall_t *local_table;
	int r = -1;

	rcu_read_lock();
	local_table = rcu_deference(table);
	if (local_table != NULL)
		r = local_table[number](args);
	rcu_read_unlock();
	return r;
}

void retract_table()
{
	syscall_t *local_table;

	//spinlock to protect the other writer
	spin_lock(&table_lock);
	local_table = table;
	rcu_assign_pointer(&table, NULL);
	spin_unlock(&table_lock);

	synchronize_rcu();
	kfree(local_table);
}

2) list protect
read:
static enum audit_state audit_filter_task(struct task_struct *tsk)
	{
		struct audit_entry *e;
		enum audit_state   state;

		rcu_read_lock();
		/* Note: audit_netlink_sem held by caller. */
		list_for_each_entry_rcu(e, &audit_tsklist, list) {
			if (audit_filter_rules(tsk, &e->rule, NULL, &state)) {
				rcu_read_unlock();
				return state;
			}
		}
		rcu_read_unlock();
		return AUDIT_BUILD_CONTEXT;
	}

del & add:
static inline int audit_del_rule(struct audit_rule *rule,
					 struct list_head *list)
	{
		struct audit_entry  *e;

		/* Do not use the _rcu iterator here, since this is the only
		 * deletion routine. */
		list_for_each_entry(e, list, list) {
			if (!audit_compare_rule(rule, &e->rule)) {
				list_del_rcu(&e->list);
				call_rcu(&e->rcu, audit_free_rule);
				return 0;
			}
		}
		return -EFAULT;		/* No matching rule */
	}

	static inline int audit_add_rule(struct audit_entry *entry,
					 struct list_head *list)
	{
		if (entry->rule.flags & AUDIT_PREPEND) {
			entry->rule.flags &= ~AUDIT_PREPEND;
			list_add_rcu(&entry->list, list);
		} else {
			list_add_tail_rcu(&entry->list, list);
		}
		return 0;
	}

replace:
static inline int audit_upd_rule(struct audit_rule *rule,
					 struct list_head *list,
					 __u32 newaction,
					 __u32 newfield_count)
	{
		struct audit_entry  *e;
		struct audit_newentry *ne;

		list_for_each_entry(e, list, list) {
			if (!audit_compare_rule(rule, &e->rule)) {
				ne = kmalloc(sizeof(*entry), GFP_ATOMIC);
				if (ne == NULL)
					return -ENOMEM;
				audit_copy_rule(&ne->rule, &e->rule);
				ne->rule.action = newaction;
				ne->rule.file_count = newfield_count;
				list_replace_rcu(&e->list, &ne->list);
				call_rcu(&e->rcu, audit_free_rule);
				return 0;
			}
		}
		return -EFAULT;		/* No matching rule */
	}


3) call_rcu
id_table_entry_t pid_table[];
process_t *pid_lookup(int pid)
{
	process_t *p

	rcu_read_lock();
	p = pid_table[pid_hash(pid)].process;
	if (p)
	atomic_inc(&p->ref);
	rcu_read_unlock();
	return p;
}

void pid_free(process *p)
{
	if (atomic_dec(&p->ref))
		free(p);
}

void pid_remove(int pid)
{
	process_t **p;

	spin_lock(&pid_table[pid_hash(pid)].lock);
	p = &pid_table[pid_hash(pid)].process;
	rcu_assign_pointer(p, NULL);
	spin_unlock(&pid_table[pid_hash(pid)].lock);
	if (*p)
		call_rcu(pid_free, *p);
}

